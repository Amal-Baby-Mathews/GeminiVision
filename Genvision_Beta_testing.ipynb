{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amal-Baby-Mathews/GeminiVision/blob/main/Genvision_Beta_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlPVRVqbrwvA",
        "outputId": "35576c5f-8880-4aa8-9980-4d93b2a3c81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.1.5-py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.0/89.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3,>=0.2 (from langgraph)\n",
            "  Downloading langchain_core-0.2.11-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.2->langgraph)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.2->langgraph)\n",
            "  Downloading langsmith-0.1.83-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (2.8.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (8.4.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (1.2.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2->langgraph)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.0.7)\n",
            "Installing collected packages: orjson, jsonpointer, h11, jsonpatch, httpcore, langsmith, httpx, langchain-core, langgraph\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.11 langgraph-0.1.5 langsmith-0.1.83 orjson-3.10.6\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph httpx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z-c_76eCrwvE"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAgE2iXkrwvH",
        "outputId": "453d5b8a-5568-40b1-94ed-818d0f8d3a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.6-py3-none-any.whl (975 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_community\n",
            "  Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.11)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.83)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-0.2.6 langchain-text-splitters-0.2.2 langchain_community-0.2.6 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkvHX5E3rwvJ"
      },
      "source": [
        "# **Code for Genvis! Let's do this!**\n",
        "Using langraph, created a langraph app to do the following:\n",
        "Greet user\n",
        "prompt user to take photo\n",
        "get photo description\n",
        "describe the photo to user ask for feedback(feedback adds to the prompt on the changes to the description of the photo)\n",
        "using the feedback to make new description of the photo\n",
        "ask user for more clarification if needed or to take new photo for analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xzpIIcQhqSdI"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "#img = PIL.Image.open('path/to/image.png')\n",
        "\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "#response = model.generate_content([\"What is in this photo?\", img])\n",
        "#print(response.text)\n",
        "import requests\n",
        "from typing import Dict, Any\n",
        "\n",
        "def google_search(api_key: str, cx: str, query: str) -> Dict[str, Any]:\n",
        "    base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "    params = {\n",
        "        'key': api_key,\n",
        "        'cx': cx,\n",
        "        'q': query\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        response.raise_for_status()\n",
        "\n",
        "api_key=userdata.get('GOOGLE_API')\n",
        "cx = '16f55fa2cee4d4e0b'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAUCT2Oxg4uD"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, TypedDict\n",
        "from langgraph.graph import StateGraph, END\n",
        "from PIL import Image\n",
        "class GraphState(TypedDict):\n",
        "    user_input: Optional[list[str]]\n",
        "    photo_description: Optional[str]\n",
        "    feedback: Optional[str]\n",
        "    llm_output: Optional[str]\n",
        "    image_path: Optional[str]\n",
        "def first_user_input(state: GraphState) -> GraphState:\n",
        "    return state\n",
        "from typing import Literal\n",
        "\n",
        "def decision(state: GraphState) -> Literal[\"Take a photo\", \"Fetch details from the image\", \"Save the photo\", \"Take a video\", \"Error\", \"Use Chat bot\", 'Search the web for particular detail']:\n",
        "      prompt = f\"\"\"You are a decision making intelligent machine inside the pipeline of an app which helps the visually impaired. The user is querying about the description of the photo or Chatting. Your task is to decide what the next step is by selecting what the output string is according to the user input. Output 'Fetch details from the image' if no user_input is found.Output the single string without quotes. Only output the exact corresponding string chosen from the following strings:\n",
        "                  'Take a photo', 'Fetch details from the image', 'Save the photo', 'Take a video', 'Search the web for particular detail', 'Use Chat bot' and 'Error'. This is the user's input: {state[\"user_input\"][-1]}\"\"\"\n",
        "      response = model.generate_content(prompt)\n",
        "      state[\"llm_output\"] = response.text.strip()\n",
        "\n",
        "      valid_outputs = {\"Take a photo\", \"Fetch details from the image\", \"Save the photo\", \"Take a video\", \"Error\", \"Use Chat bot\",'Search the web for particular detail'}\n",
        "\n",
        "      if state[\"llm_output\"] in valid_outputs:\n",
        "          return state[\"llm_output\"]  # Return the string as a Literal\n",
        "      else:\n",
        "          state[\"llm_output\"]=\"Error\"\n",
        "          return state[\"llm_output\"]\n",
        "\n",
        "def take_photo(state: GraphState) -> GraphState:\n",
        "    # Assuming some logic here to take a photo and update the state\n",
        "    state[\"llm_output\"] = \"Photo taken\"\n",
        "    return state\n",
        "def fetch_details(state: GraphState) -> GraphState:\n",
        "    # Assuming some logic here to fetch details from the photo and update the state\n",
        "    if state[\"image_path\"] is not None:\n",
        "      img = Image.open(state[\"image_path\"])\n",
        "      state[\"photo_description\"]=model.generate_content([f\"You are an assistant for a blind user. You are very kind and accommodating.Refer to the image as view.The user's input is {state['user_input'][-1]}.Describe the view according to the user's request: \" , img]).text\n",
        "      state[\"llm_output\"] = state[\"photo_description\"]\n",
        "    else:\n",
        "      state[\"llm_output\"] = \"Could not find the image\"\n",
        "    return state\n",
        "def chat_bot(state: GraphState) -> GraphState:\n",
        "    response=model.generate_content([f\"You are an assistant for a blind user. You are very kind. Answer the User's query: {state['user_input'][-1]}\"])\n",
        "    state[\"llm_output\"]=response.text\n",
        "    return state\n",
        "def web_search(state:GraphState) -> GraphState:\n",
        "    search_list=[f\"Generate a short single line Web search query to answer user's query: {state['user_input'][-1]}. Use the image to understand the query if necessary\"]\n",
        "    if state[\"image_path\"] is not None:\n",
        "      img = Image.open(state[\"image_path\"])\n",
        "      search_list.append(img)\n",
        "    #Logic for searching the internet\n",
        "    search_prompt=model.generate_content(search_list)\n",
        "    search_prompt=search_prompt.text.strip('\"')\n",
        "    #Implement Search using Google Custom API:\n",
        "    search_results = google_search(api_key, cx, search_prompt)\n",
        "    print(search_results)\n",
        "    result=\"\"\n",
        "    for item in search_results.get('items', [])[:3]:\n",
        "        result+=f\"Title: {item['title']}\\nSnippet: {item['snippet']}\\n\\n\"\n",
        "    if result==\"\":\n",
        "      result=\"No results found\"\n",
        "    state[\"feedback\"]=result\n",
        "    result=model.generate_content([f\"You are an assistant for a blind user. You are very straightforward.Refer to the search result:{result}, and try to answer user's query: {state['user_input'][-1]}. \"])\n",
        "    state[\"llm_output\"]=result.text\n",
        "    return state\n",
        "def save_photo(state: GraphState) -> GraphState:\n",
        "    # Assuming some logic here to save the photo and update the state\n",
        "    state[\"llm_output\"] = \"Photo saved\"\n",
        "    return state\n",
        "def take_video(state: GraphState) -> GraphState:\n",
        "    # Assuming some logic here to take a video and update the state\n",
        "    state[\"llm_output\"] = \"Video taken\"\n",
        "    return state\n",
        "def error(state: GraphState) -> GraphState:\n",
        "    # Assuming some logic here to handle errors and update the state\n",
        "    state[\"llm_output\"]= \"Some internal error occured Please retry\"\n",
        "    return state\n",
        "workflow = StateGraph(GraphState)\n",
        "workflow.add_node(\"first_user_input\", first_user_input)\n",
        "\n",
        "workflow.add_node(\"take_photo\", take_photo)\n",
        "workflow.add_node(\"fetch_details\", fetch_details)\n",
        "workflow.add_node(\"save_photo\", save_photo)\n",
        "workflow.add_node(\"take_video\", take_video)\n",
        "workflow.add_node(\"error\", error)\n",
        "workflow.add_node(\"web_search\", web_search)\n",
        "workflow.add_node(\"chat_bot\", chat_bot)\n",
        "workflow.set_entry_point(\"first_user_input\")\n",
        "workflow.add_edge(\"take_photo\", END)\n",
        "workflow.add_edge(\"fetch_details\", END)\n",
        "workflow.add_edge(\"save_photo\", END)\n",
        "workflow.add_edge(\"take_video\", END)\n",
        "workflow.add_conditional_edges(\n",
        "    \"first_user_input\",\n",
        "    decision,\n",
        "    {\n",
        "        \"Take a photo\": \"take_photo\",\n",
        "        \"Fetch details from the image\": \"fetch_details\",\n",
        "        \"Save the photo\": \"save_photo\",\n",
        "        \"Take a video\": \"take_video\",\n",
        "        \"Error\": \"error\",\n",
        "        \"Use Chat bot\": \"chat_bot\",\n",
        "        \"Search the web for particular detail\": \"web_search\",\n",
        "    },\n",
        ")\n",
        "\n",
        "app = workflow.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Be3Zet5Yl0VK",
        "outputId": "0fa37127-2895-4c2b-9b2a-51167949957f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'user_input': ['What is the age of enthiran', 'Hi', 'what is in this image?'],\n",
              " 'photo_description': 'Sure, I can help you with that.  The view is a park with a brick pathway that curves through the green grass. There is a wooden bench on the left side of the path.  There are trees all around the path and a gazebo in the background. It looks like a lovely place to relax and enjoy the fresh air. \\n',\n",
              " 'feedback': None,\n",
              " 'llm_output': 'Sure, I can help you with that.  The view is a park with a brick pathway that curves through the green grass. There is a wooden bench on the left side of the path.  There are trees all around the path and a gazebo in the background. It looks like a lovely place to relax and enjoy the fresh air. \\n',\n",
              " 'image_path': '/content/360_F_76257590_OMqEbhnSnz30cLj6xAG511xSZrJabcsq.jpg'}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke({\"user_input\":[\"What is the age of enthiran\",\"Hi\",\"what is in this image?\"],\"photo_description\":None, \"feedback\": None, \"llm_output\":None, \"image_path\":\"/content/360_F_76257590_OMqEbhnSnz30cLj6xAG511xSZrJabcsq.jpg\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ycnPmqM5eN10"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, TypedDict, Literal\n",
        "from langgraph.graph import StateGraph, END\n",
        "from PIL import Image\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    user_input: Optional[list[str]]\n",
        "    photo_description: Optional[str]\n",
        "    feedback: Optional[str]\n",
        "    llm_output: Optional[str]\n",
        "    image_path: Optional[str]\n",
        "    chat_history: Optional[list[str]]\n",
        "\n",
        "def get_last_two_chats(chat_history: list[str]) -> str:\n",
        "    his=\" \".join(chat_history[-4:]) if len(chat_history) >= 4 else \" \".join(chat_history)\n",
        "    return his\n",
        "\n",
        "def first_user_input(state: GraphState) -> GraphState:\n",
        "    if state.get(\"chat_history\") is None:\n",
        "        state[\"chat_history\"] = []\n",
        "    return state\n",
        "\n",
        "def decision(state: GraphState) -> Literal[\"Take a photo\", \"Fetch details from the image\", \"Save the photo\", \"Take a video\", \"Error\", \"Use Chat bot\", 'Search the web for particular detail']:\n",
        "    last_two_chats = get_last_two_chats(state[\"chat_history\"])\n",
        "    prompt = f\"\"\"You are a decision making intelligent machine inside the pipeline of an app which helps the visually impaired. The user is querying about the description of the photo or Chatting. Your task is to decide what the next step is by selecting what the output string is according to the user input. Output 'Fetch details from the image' if no user_input is found. Output the single string without quotes. Only output the exact corresponding string chosen from the following strings:\n",
        "                'Take a photo', 'Fetch details from the image', 'Save the photo', 'Take a video', 'Search the web for particular detail', 'Use Chat bot' and 'Error'.\n",
        "                This is the user's input: {state[\"user_input\"][-1]}\n",
        "                Recent chat history: {last_two_chats}\"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    state[\"llm_output\"] = response.text.strip()\n",
        "\n",
        "    valid_outputs = {\"Take a photo\", \"Fetch details from the image\", \"Save the photo\", \"Take a video\", \"Error\", \"Use Chat bot\", 'Search the web for particular detail'}\n",
        "\n",
        "    if state[\"llm_output\"] in valid_outputs:\n",
        "        return state[\"llm_output\"]\n",
        "    else:\n",
        "        state[\"llm_output\"] = \"Error\"\n",
        "        return state[\"llm_output\"]\n",
        "\n",
        "def take_photo(state: GraphState) -> GraphState:\n",
        "    state[\"llm_output\"] = \"Photo taken\"\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def fetch_details(state: GraphState) -> GraphState:\n",
        "    if state[\"image_path\"] is not None:\n",
        "        img = Image.open(state[\"image_path\"])\n",
        "        last_two_chats = get_last_two_chats(state[\"chat_history\"])\n",
        "        state[\"photo_description\"] = model.generate_content([\n",
        "            f\"You are an assistant for a blind user. You are very kind and accommodating. Refer to the image as view. The user's input is {state['user_input'][-1]}. Recent chat history: {last_two_chats}. Describe the view according to the user's request: \",\n",
        "            img\n",
        "        ]).text\n",
        "        state[\"llm_output\"] = state[\"photo_description\"]\n",
        "    else:\n",
        "        state[\"llm_output\"] = \"Could not find the image\"\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def chat_bot(state: GraphState) -> GraphState:\n",
        "    last_two_chats = get_last_two_chats(state[\"chat_history\"])\n",
        "    prompt=last_two_chats\n",
        "    response = model.generate_content([f\"You are an assistant for a blind user. You are very kind. Recent chat history: {last_two_chats}. Answer the User's query: {state['user_input'][-1]}\"])\n",
        "    state[\"llm_output\"] = response.text\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def web_search(state: GraphState) -> GraphState:\n",
        "    last_two_chats = get_last_two_chats(state[\"chat_history\"])\n",
        "    search_list = [f\"Generate a short single line Web search query to answer user's query: {state['user_input'][-1]}. Recent chat history: {last_two_chats}. Use the image to understand the query if necessary\"]\n",
        "    if state[\"image_path\"] is not None:\n",
        "        img = Image.open(state[\"image_path\"])\n",
        "        search_list.append(img)\n",
        "    search_prompt = model.generate_content(search_list)\n",
        "    search_prompt = search_prompt.text.strip('\"')\n",
        "    search_results = google_search(api_key, cx, search_prompt)\n",
        "    result = \"\"\n",
        "    for item in search_results.get('items', [])[:3]:\n",
        "        result += f\"Title: {item['title']}\\nSnippet: {item['snippet']}\\n\\n\"\n",
        "    if result == \"\":\n",
        "        result = \"No results found\"\n",
        "    state[\"feedback\"] = result\n",
        "    result = model.generate_content([f\"You are an assistant for a blind user. You are very straightforward. Refer to the search result:{result}, and try to answer user's query: {state['user_input'][-1]}. Recent chat history: {last_two_chats}\"])\n",
        "    state[\"llm_output\"] = result.text\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def save_photo(state: GraphState) -> GraphState:\n",
        "    state[\"llm_output\"] = \"Photo saved\"\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def take_video(state: GraphState) -> GraphState:\n",
        "    state[\"llm_output\"] = \"Video taken\"\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "def error(state: GraphState) -> GraphState:\n",
        "    state[\"llm_output\"] = \"Some internal error occurred. Please retry\"\n",
        "    state[\"chat_history\"].append(f\"Assistant: {state['llm_output']}\")\n",
        "    return state\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "workflow.add_node(\"first_user_input\", first_user_input)\n",
        "\n",
        "workflow.add_node(\"take_photo\", take_photo)\n",
        "workflow.add_node(\"fetch_details\", fetch_details)\n",
        "workflow.add_node(\"save_photo\", save_photo)\n",
        "workflow.add_node(\"take_video\", take_video)\n",
        "workflow.add_node(\"error\", error)\n",
        "workflow.add_node(\"web_search\", web_search)\n",
        "workflow.add_node(\"chat_bot\", chat_bot)\n",
        "workflow.set_entry_point(\"first_user_input\")\n",
        "workflow.add_edge(\"take_photo\", END)\n",
        "workflow.add_edge(\"fetch_details\", END)\n",
        "workflow.add_edge(\"save_photo\", END)\n",
        "workflow.add_edge(\"take_video\", END)\n",
        "workflow.add_conditional_edges(\n",
        "    \"first_user_input\",\n",
        "    decision,\n",
        "    {\n",
        "        \"Take a photo\": \"take_photo\",\n",
        "        \"Fetch details from the image\": \"fetch_details\",\n",
        "        \"Save the photo\": \"save_photo\",\n",
        "        \"Take a video\": \"take_video\",\n",
        "        \"Error\": \"error\",\n",
        "        \"Use Chat bot\": \"chat_bot\",\n",
        "        \"Search the web for particular detail\": \"web_search\",\n",
        "    },\n",
        ")\n",
        "\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "zDJyYo42jTV0",
        "outputId": "3612d5e1-d8e2-4098-d328-dec5c46ceb09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the chat system. Type 'exit' to end the conversation.\n",
            "User: find me famous gazebos\n",
            "Include an image? (yes/no): \n",
            "User: find me famous gazebos\n",
            "User: find me famous gazebos\n",
            "Assistant: Okay, I found some information about famous gazebos.  There are gazebos in Ghent, Belgium, Halifax, Nova Scotia, and Bergen, Norway. There is also a gazebo in Brussels, but it's not clear if it is famous or not.  One of these might be what you're looking for. \n",
            "\n",
            "{'user_input': ['find me famous gazebos'], 'photo_description': None, 'feedback': \"Title: Top 5 Gazebos | Stephen Travels\\nSnippet: Top 5 Gazebos · Gazebo, Citadel Park, Ghent, Belgium · Bandstand, Halifax Public Gardens, Halifax, Nova Scotia · Music Pavilion, Bergen, Norway · Gazebo, Brussels\\xa0...\\n\\nTitle: The Story of the Sautee Nacoochee Indian Mound\\nSnippet: He shaved two feet off the top of the mounds (older documents show that the mound used to be sixteen feet tall) and planted the now (in)famous gazebo on top.\\n\\nTitle: Gazebos—you don't have to be rich and famous to live like you are ...\\nSnippet: Jan 30, 2014 ... There is also a style of Gazebo to fit almost any architecture. Styles hail from just about everywhere including the South Pacific, the UK, and\\xa0...\\n\\n\", 'llm_output': \"Okay, I found some information about famous gazebos.  There are gazebos in Ghent, Belgium, Halifax, Nova Scotia, and Bergen, Norway. There is also a gazebo in Brussels, but it's not clear if it is famous or not.  One of these might be what you're looking for. \\n\", 'image_path': None, 'chat_history': ['User: find me famous gazebos', \"Assistant: Okay, I found some information about famous gazebos.  There are gazebos in Ghent, Belgium, Halifax, Nova Scotia, and Bergen, Norway. There is also a gazebo in Brussels, but it's not clear if it is famous or not.  One of these might be what you're looking for. \\n\"]}\n",
            "User: is there one in the photo?\n",
            "Include an image? (yes/no): yes\n",
            "Enter image path: \n",
            "User: find me famous gazebos Assistant: Okay, I found some information about famous gazebos.  There are gazebos in Ghent, Belgium, Halifax, Nova Scotia, and Bergen, Norway. There is also a gazebo in Brussels, but it's not clear if it is famous or not.  One of these might be what you're looking for. \n",
            " Assistant: Okay, I found some information about famous gazebos.  There are gazebos in Ghent, Belgium, Halifax, Nova Scotia, and Bergen, Norway. There is also a gazebo in Brussels, but it's not clear if it is famous or not.  One of these might be what you're looking for. \n",
            " User: is there one in the photo?\n",
            "User: find me famous gazebos Assistant: Okay, I found some information about famous gazebos.  There are gazebos in Ghent, Belgium, Halifax, Nova Scotia, and Bergen, Norway. There is also a gazebo in Brussels, but it's not clear if it is famous or not.  One of these might be what you're looking for. \n",
            " Assistant: Okay, I found some information about famous gazebos.  There are gazebos in Ghent, Belgium, Halifax, Nova Scotia, and Bergen, Norway. There is also a gazebo in Brussels, but it's not clear if it is famous or not.  One of these might be what you're looking for. \n",
            " User: is there one in the photo?\n",
            "Assistant: Yes, there is a gazebo in the view.  It is located in the middle of the photo, behind a brick walkway.  It is a white gazebo with a wooden roof and is surrounded by tall trees.  You can see the gazebo between some trees in the background.  Let me know if you need more information.  \n",
            "\n",
            "{'user_input': ['is there one in the photo?'], 'photo_description': 'Yes, there is a gazebo in the view.  It is located in the middle of the photo, behind a brick walkway.  It is a white gazebo with a wooden roof and is surrounded by tall trees.  You can see the gazebo between some trees in the background.  Let me know if you need more information.  \\n', 'feedback': None, 'llm_output': 'Yes, there is a gazebo in the view.  It is located in the middle of the photo, behind a brick walkway.  It is a white gazebo with a wooden roof and is surrounded by tall trees.  You can see the gazebo between some trees in the background.  Let me know if you need more information.  \\n', 'image_path': '/content/360_F_76257590_OMqEbhnSnz30cLj6xAG511xSZrJabcsq.jpg', 'chat_history': ['User: find me famous gazebos', \"Assistant: Okay, I found some information about famous gazebos.  There are gazebos in Ghent, Belgium, Halifax, Nova Scotia, and Bergen, Norway. There is also a gazebo in Brussels, but it's not clear if it is famous or not.  One of these might be what you're looking for. \\n\", \"Assistant: Okay, I found some information about famous gazebos.  There are gazebos in Ghent, Belgium, Halifax, Nova Scotia, and Bergen, Norway. There is also a gazebo in Brussels, but it's not clear if it is famous or not.  One of these might be what you're looking for. \\n\", 'User: is there one in the photo?', 'Assistant: Yes, there is a gazebo in the view.  It is located in the middle of the photo, behind a brick walkway.  It is a white gazebo with a wooden roof and is surrounded by tall trees.  You can see the gazebo between some trees in the background.  Let me know if you need more information.  \\n']}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-20fb7500692d>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mrun_chat_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-20fb7500692d>\u001b[0m in \u001b[0;36mrun_chat_loop\u001b[0;34m(app)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Get user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "def run_chat_loop(app):\n",
        "    state = {\n",
        "        \"user_input\": [],\n",
        "        \"photo_description\": None,\n",
        "        \"feedback\": None,\n",
        "        \"llm_output\": None,\n",
        "        \"image_path\": None,\n",
        "        \"chat_history\": []\n",
        "    }\n",
        "\n",
        "    print(\"Welcome to the chat system. Type 'exit' to end the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_message = input(\"User: \").strip()\n",
        "        if user_message.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Option to include image path\n",
        "        include_image = input(\"Include an image? (yes/no): \").lower().strip() == 'yes'\n",
        "        if include_image:\n",
        "            image_path = input(\"Enter image path: \").strip()\n",
        "            if not image_path:\n",
        "                image_path=\"/content/360_F_76257590_OMqEbhnSnz30cLj6xAG511xSZrJabcsq.jpg\"\n",
        "            state[\"image_path\"] = image_path\n",
        "        else:\n",
        "            state[\"image_path\"] = None\n",
        "\n",
        "        # Update user_input and chat_history\n",
        "        state[\"user_input\"] = [user_message]\n",
        "        state[\"chat_history\"].append(f\"User: {user_message}\")\n",
        "\n",
        "        # Keep only the last two exchanges in chat_history\n",
        "        if len(state[\"chat_history\"]) > 4:\n",
        "            state[\"chat_history\"] = state[\"chat_history\"][-4:]\n",
        "\n",
        "        # Invoke the app\n",
        "        result = app.invoke(state)\n",
        "\n",
        "        # Extract the assistant's response\n",
        "        assistant_response = result.get(\"llm_output\", \"No response generated.\")\n",
        "\n",
        "        # Print the assistant's response\n",
        "        print(f\"Assistant: {assistant_response}\")\n",
        "        print(result)\n",
        "        # Update chat_history with assistant's response\n",
        "        state[\"chat_history\"].append(f\"Assistant: {assistant_response}\")\n",
        "\n",
        "        # Clear previous user input and llm_output\n",
        "        state[\"user_input\"] = []\n",
        "        state[\"llm_output\"] = None\n",
        "\n",
        "    print(\"Chat ended. Thank you for using the system!\")\n",
        "\n",
        "# Usage\n",
        "run_chat_loop(app)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqnqWXQWrYF8"
      },
      "outputs": [],
      "source": [
        "query = 'who is Adam driver?'\n",
        "\n",
        "try:\n",
        "    search_results = google_search(api_key, cx, query)\n",
        "    for item in search_results.get('items', []):\n",
        "        print(f\"Title: {item['title']}\")\n",
        "        print(f\"Snippet: {item['snippet']}\")\n",
        "        print(f\"Link: {item['link']}\")\n",
        "        print(\"\\n\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STF5iItEz0kY"
      },
      "outputs": [],
      "source": [
        "cont=True\n",
        "user_input=\"Take that photo\"\n",
        "cache={\"user_input\":None,\"photo_description\":None, \"feedback\": None, \"llm_output\":None,\"image_path\": \"/content/360_F_76257590_OMqEbhnSnz30cLj6xAG511xSZrJabcsq.jpg\"}\n",
        "while cont is True:\n",
        "  user_input=input(\"Enter your input: \")\n",
        "  if user_input==\"x\":\n",
        "    cont=False\n",
        "  cache[\"user_input\"]=user_input\n",
        "  cache=app.invoke(cache)\n",
        "  print(cache)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVg0kTLDxr4E7E2tZRA3S5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}